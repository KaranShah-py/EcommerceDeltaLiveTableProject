{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ffc25c8e-3df4-409b-ba1c-f01c0b3dc0aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Address Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7cb1ec5-b34e-40b1-bb80-0cbe3e8a00dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Process the Addresses Data -- Using Python Implementation\n",
    "- Ingest the data into the data lakehouse - bronze address\n",
    "- Perform the data quality checks and transform the data as required - silver_address_clean\n",
    "- Apply the changes to the address data - silver_address_scd1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "690a340a-5905-41be-83c0-4def9b39f8be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###0. Importing the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50372913-219f-4e0f-9d32-54e507f8b645",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Libraries and Modules"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "    <html>\n",
       "      <div style=\"font-size:18px\">\n",
       "        The Delta Live Tables (DLT) module is not supported on this cluster.\n",
       "        You should either <a href=\"?o=4039340823107408#joblist/pipelines/create?initialSource=%2FUsers%2Fphotosdrive599%40gmail.com%2FDataEngineeringDeltaLiveProject%2FProcessingAddressesData&redirectNotebookId=2453181026928550\">create a new pipeline</a> or use an existing pipeline to run DLT code.\n",
       "      </div>\n",
       "    </html>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-8662675776394380>, line 1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mdlt\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'dlt'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "ModuleNotFoundError",
        "evalue": "No module named 'dlt'"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>ModuleNotFoundError</span>: No module named 'dlt'"
       },
       "removedWidgets": [],
       "sqlProps": {
        "breakingChangeInfo": null,
        "errorClass": "NOTEBOOK_USER_ERROR",
        "pysparkCallSite": null,
        "pysparkFragment": null,
        "pysparkSummary": null,
        "sqlState": "KD00G",
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
        "File \u001B[0;32m<command-8662675776394380>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mdlt\u001B[39;00m\n",
        "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'dlt'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import dlt\n",
    "import spark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "baa2a482-e145-4f15-8a3a-1a37a0ca85eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###1. Ingest the data into the data lakehouse - Bronze Address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5de8e138-b056-4fae-b83e-25b7b271a743",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Creating the Bronze Address Table"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>message</th></tr></thead><tbody><tr><td>This Delta Live Tables query is syntactically valid, but you must create a pipeline in order to define and populate your table.</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "This Delta Live Tables query is syntactically valid, but you must create a pipeline in order to define and populate your table."
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "message",
            "nullable": true,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 4
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "message",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "# CREATE OR REFRESH STREAMING TABLE ecommercedataset.bronzelayer.customersbronzetable\n",
    "# COMMENT 'Raw Addresses data ingested from the source system operational data'\n",
    "# TBLPROPERTIES ('quality' = 'bronze')\n",
    "# AS \n",
    "# SELECT *\n",
    "# FROM STREAM cloud_files(\n",
    "#     '/Volumes/ecommercedataset/landinglayer/landingdata/',\n",
    "#     'json'\n",
    "#   );\n",
    "'''\n",
    "\n",
    "# Pyspark version\n",
    "@dlt.table(\n",
    "  name = 'AddressesBronzeTable', \n",
    "  comment = 'Raw Addresses data ingested from the source system operational data', \n",
    "  table_properties = {'quality': 'bronze'}\n",
    ")\n",
    "def CreateAddressesBronzeTable():\n",
    "  return \n",
    "  (\n",
    "    spark.readStream\n",
    "         .format(\"cloudFiles\")\n",
    "         .option(\"cloudFiles.format\", \"csv\")\n",
    "         .option(\"cloudFiles.inferColumnTypes\", \"true\"), \n",
    "         .load(\"/Volumes/ecommercedataset/landinglayer/landingdata/Addresses\")\n",
    "         .select(\n",
    "           \"*\", \n",
    "           F.col(\"_metadata.file_path\").alias(\"input_file_path\"), \n",
    "           F.current_timestamp().alias(\"ingest_timestamp\")\n",
    "         )\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed14b5a0-7ac8-4ee3-96a3-4af5103ea9ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###2. Specifying the Expectations for the addresses table \n",
    "> Silver addresses Clean (DQ Rules)\n",
    "1. Fail if customer_id is NULL\n",
    "2. Drop records with address_line_1 as NULL\n",
    "3. Warn if postcode is not 5 digits \n",
    "\n",
    "> Silver addresses Clean (Transformations)\n",
    "1. CAST date_of_birth to DATE\n",
    "\n",
    "- Silver_Addresses is a TYPE_2 Dimension table with primary key \n",
    "- as customer_id. Apply the changes to the addresses data based on the \n",
    "- created_date and create history for each address added for the customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24596611-dd6a-416d-a9ad-9841331b0aa4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Implementing Expectations for Addresses Clean"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "%sql\n",
    "CREATE OR REFRESH STREAMING TABLE ecommercedataset.silverlayer.AddressesSilverClean\n",
    "-- expectations\n",
    "(\n",
    "  -- Mention the Expectations here\n",
    "  CONSTRAINT valid_customer_id EXPECT(CUSTOMER_ID IS NOT NULL) ON VIOLATION FAIL UPDATE, \n",
    "  CONSTRAINT valid_address EXPECT(ADDRESS_LINE_1 IS NOT NULL) ON VIOLATION DROP ROW, \n",
    "  CONSTRAINT valid_postcode EXPECT(LENGTH(POSTCODE) = 5)\n",
    ")\n",
    "-- comment \n",
    "COMMENT 'Table created with Expectations and Data quality checks'\n",
    "TBLPROPERTIES ('quality', 'silver')\n",
    "AS \n",
    "SELECT\n",
    "  customer_id, \n",
    "  address_line_1, \n",
    "  city, \n",
    "  state, \n",
    "  postcode, \n",
    "  CAST(created_date AS DATE) AS CREATED_DATE  \n",
    "FROM STREAM(LIVE.ecommercedataset.bronzelayer.AddressesBronze);\n",
    "'''\n",
    "\n",
    "# Pyspark Code \n",
    "@dlt.table(\n",
    "    # Specifying the properties of the table =\n",
    "    name = 'CustomerSilverClean', \n",
    "    comment = 'Table created with Expectations and Data quality checks', \n",
    "    table_properties  = {'quality': 'silver'}\n",
    ")\n",
    "# Sepcifying the expectations \n",
    "@expect_or_fail(\"valid_customer_id\", \"customer_id is not null\") # expect or fail\n",
    "@expect_or_drop(\"valid_address\", \"address_line_1 is not null\") # expect or drop\n",
    "@expect(\"valid_postcode\", \"LENGTH(postcode) = 5\") # Expect or want\n",
    "def CustomerSilverClean():\n",
    "    return \n",
    "    (\n",
    "        spark.readStream(\"LIVE.ecommercedataset.bronzeLayer.AddressesBronze\")\n",
    "             .select(\n",
    "                 \"customer_id\", \n",
    "                 \"address_line_1\", \n",
    "                 \"city\", \n",
    "                 \"state\",\n",
    "                 \"postcode\", \n",
    "                 \"country\", \n",
    "                 F.col(\"created_date\").cast(\"date\")\n",
    "             )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77a6985f-af4b-445b-a67f-0c5baf2af17e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###3. Apply changes to the Customers data - Silver Customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f9a0bcc-ec41-4fff-a58d-0e4960050510",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 1: Creating streaming table for SCD Type 2"
    }
   },
   "outputs": [],
   "source": [
    "# Since we need to create SCD we first need to create streaming table \n",
    "'''\n",
    "# SQL \n",
    "    CREATE OR REFRESH STREAMING TABLE ecommercedataset.silverlayer.AddressesSilverClean\n",
    "    COMMENT 'SCD Type 1 Customers Data'\n",
    "    TBLPROPERTIES ('quality' = 'silver');\n",
    "'''\n",
    "\n",
    "# pyspark\n",
    "dlt.create_streaming_table(\n",
    "    name = \"ecommercedataset.silverlayer.AddressesSilver\", \n",
    "    comment = \"SCD Type 2 Addresses Data\"\n",
    "    table_properties = {\"quality\", \"silver\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1313ec1-b9ba-4430-aabb-acf5609726b2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 2: Applying SCD TYPE 2 on streaming table"
    }
   },
   "outputs": [],
   "source": [
    "# Applying changes to the streaming table created\n",
    "'''\n",
    "# sql\n",
    "APPLY CHANGES INTO LIVE(ecommercedataset.silverlayer.AddressesSilver)\n",
    "FROM STREAM(LIVE.AddressesSilverClean)\n",
    "KEYS (customer_id)\n",
    "SEQUENCE BY created_date\n",
    "STORED AS SCD TYPE 1; \n",
    "'''\n",
    "@dlt.apply_changes(\n",
    "    target = \"ecommercedataset.silverlayer.AddressesSilver\", \n",
    "    source = \"AddressesSilverClean\"\n",
    "    keys = [\"customer_id\"], \n",
    "    sequence_by = \"created_date\", \n",
    "    stored_as_scd_type = 2\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ProcessingAddressesData",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}